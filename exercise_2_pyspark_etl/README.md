# ExercÃ­cio 2: ETL PySpark com Modelagem HistÃ³rica

## ğŸ“Œ Contexto

Este exercÃ­cio Ã© **significativamente mais complexo** que o primeiro, pois trabalha com:

- **Modelo de Eventos (Event Sourcing)**: Todas as alteraÃ§Ãµes sÃ£o registradas
- **Chegada AssÃ­ncrona**: As 3 tabelas nÃ£o sÃ£o salvas simultaneamente
- **Rastreabilidade Temporal**: Necessidade de navegar entre perÃ­odos
- **IdempotÃªncia**: Reprocessar deve gerar o mesmo resultado
- **SCD Type 2**: Manter histÃ³rico completo de mudanÃ§as

---

## ğŸ—‚ï¸ Modelo de Dados Fonte

### Tabela: `purchase` (Eventos)

Registra **todas as alteraÃ§Ãµes** que ocorrem em uma compra.

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `purchase_id` | BIGINT | ID da compra |
| `buyer_id` | BIGINT | ID do comprador |
| `purchase_relation_id` | BIGINT | FK para relacionar com itens |
| `order_date` | DATE | Data do pedido |
| `release_date` | DATE | Data de pagamento confirmado |
| `producer_id` | BIGINT | ID do produtor |
| `purchase_value` | DECIMAL(10,2) | Valor da compra |
| `transaction_date` | DATE | **Data em que o evento foi salvo no banco** |

**CaracterÃ­sticas:**
- Mesma `purchase_id` pode aparecer mÃºltiplas vezes (histÃ³rico de mudanÃ§as)
- `transaction_date` determina quando a mudanÃ§a ocorreu
- NÃ£o Ã© tabela corrente - Ã© tabela de eventos (CDC)

### Tabela: `product_item` (Eventos)

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `product_item_id` | BIGINT | ID do item |
| `purchase_relation_id` | BIGINT | FK para compra |
| `product_id` | BIGINT | ID do produto |
| `item_value` | DECIMAL(10,2) | Valor do item |
| `transaction_date` | DATE | Data do evento |

### Tabela: `purchase_extra_info` (Eventos)

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `purchase_extra_info_id` | BIGINT | ID da informaÃ§Ã£o extra |
| `purchase_relation_id` | BIGINT | FK para compra |
| `subsidiary` | VARCHAR(50) | NATIONAL ou INTERNATIONAL |
| `transaction_date` | DATE | Data do evento |

---

## âš ï¸ Comportamento AssÃ­ncrono (CRÃTICO!)

### Exemplo Real do Teste:

```
Compra ID: 55
Order Date: 2023-01-20

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ transaction_dateâ”‚ Tabela que Recebeu Evento                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2023-01-20      â”‚ âœ… purchase (compra criada)                    â”‚
â”‚ 2023-01-20      â”‚ âœ… product_item (item registrado)              â”‚
â”‚ 2023-01-20      â”‚ âŒ purchase_extra_info (NÃƒO CHEGOU!)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2023-01-23      â”‚ âŒ purchase (sem mudanÃ§as)                     â”‚
â”‚ 2023-01-23      â”‚ âŒ product_item (sem mudanÃ§as)                 â”‚
â”‚ 2023-01-23      â”‚ âœ… purchase_extra_info (chegou 3 dias depois!) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2023-02-05      â”‚ âœ… purchase (buyer_id alterado)                â”‚
â”‚ 2023-02-05      â”‚ âŒ product_item (sem mudanÃ§as)                 â”‚
â”‚ 2023-02-05      â”‚ âŒ purchase_extra_info (sem mudanÃ§as)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ConsequÃªncias:

1. **Full Outer Join obrigatÃ³rio**: Evento pode chegar em qualquer tabela primeiro
2. **Forward Fill necessÃ¡rio**: Repetir valores anteriores quando nÃ£o hÃ¡ atualizaÃ§Ã£o
3. **Complexidade na detecÃ§Ã£o de mudanÃ§as**: O que realmente mudou vs o que Ã© repetiÃ§Ã£o?

---

## ğŸ¯ Requisitos do ExercÃ­cio

| # | Requisito | DescriÃ§Ã£o | Status |
|---|-----------|-----------|--------|
| 1 | **Modelagem HistÃ³rica** | Tabela final deve manter rastreabilidade completa | âœ… |
| 2 | **Processamento D-1** | Processar apenas eventos de D-1 a cada execuÃ§Ã£o | âœ… |
| 3 | **Particionamento** | Particionar por `transaction_date` | âœ… |
| 4 | **IdempotÃªncia** | Reprocessar gera sempre o mesmo resultado | âœ… |
| 5 | **Time Travel** | Permitir consultar GMV em qualquer ponto no tempo | âœ… |
| 6 | **Assincronismo** | Tratar chegada fora de ordem das 3 tabelas | âœ… |
| 7 | **Forward Fill** | Repetir dados quando tabela nÃ£o atualiza | âœ… |
| 8 | **Dados Correntes** | Facilitar consulta do Ãºltimo estado | âœ… |
| 9 | **GMV AuditÃ¡vel** | Garantir que GMV nÃ£o muda com reprocessamento | âœ… |

---

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

### Diagrama de Fluxo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAMADA DE INGESTÃƒO                         â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚purchase â”‚     â”‚product_item  â”‚     â”‚purchase_extra   â”‚   â”‚
â”‚  â”‚(events) â”‚     â”‚  (events)    â”‚     â”‚  _info (events) â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                 â”‚                       â”‚            â”‚
â”‚       â”‚  Filtrar por transaction_date = D-1     â”‚            â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORMAÃ‡ÃƒO (PySpark)                    â”‚
â”‚                                                               â”‚
â”‚  1ï¸âƒ£ Leitura de Eventos D-1                                   â”‚
â”‚     - Ler purchase WHERE transaction_date = :process_date    â”‚
â”‚     - Ler product_item WHERE transaction_date = :process_dateâ”‚
â”‚     - Ler purchase_extra_info WHERE ...                      â”‚
â”‚                                                               â”‚
â”‚  2ï¸âƒ£ Full Outer Join                                          â”‚
â”‚     - Join por purchase_relation_id                          â”‚
â”‚     - Manter registros mesmo se apenas 1 tabela atualizou    â”‚
â”‚                                                               â”‚
â”‚  3ï¸âƒ£ Buscar Estado Anterior (D-2, D-3, ...)                   â”‚
â”‚     - Para cada purchase_id sem atualizaÃ§Ã£o em alguma tabela â”‚
â”‚     - Buscar Ãºltimo valor conhecido na tabela histÃ³rica      â”‚
â”‚     - Forward fill: repetir valor anterior                   â”‚
â”‚                                                               â”‚
â”‚  4ï¸âƒ£ DetecÃ§Ã£o de MudanÃ§a Real                                 â”‚
â”‚     - Calcular hash MD5 do registro completo                 â”‚
â”‚     - Comparar com hash do registro anterior                 â”‚
â”‚     - Inserir nova linha apenas se houver mudanÃ§a            â”‚
â”‚                                                               â”‚
â”‚  5ï¸âƒ£ Aplicar SCD Type 2                                       â”‚
â”‚     - effective_date: transaction_date da mudanÃ§a            â”‚
â”‚     - end_date: transaction_date da prÃ³xima mudanÃ§a          â”‚
â”‚     - is_current: TRUE para versÃ£o mais recente              â”‚
â”‚                                                               â”‚
â”‚  6ï¸âƒ£ Atualizar Flags                                          â”‚
â”‚     - Atualizar is_current das linhas antigas para FALSE     â”‚
â”‚     - Atualizar end_date das linhas antigas                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CAMADA DE ARMAZENAMENTO                          â”‚
â”‚                                                               â”‚
â”‚           fact_purchase_history                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ purchase_id | effective_date | end_date | ..â”‚             â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
â”‚  â”‚     55      |  2023-01-20    |2023-01-23|.. â”‚  <- v1      â”‚
â”‚  â”‚     55      |  2023-01-23    |2023-02-05|.. â”‚  <- v2      â”‚
â”‚  â”‚     55      |  2023-02-05    |   NULL   |.. â”‚  <- v3 (now)â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                               â”‚
â”‚  PARTITIONED BY (transaction_date DATE)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Modelagem: `fact_purchase_history`

### DDL (Data Definition Language)

```sql
CREATE TABLE fact_purchase_history (
    -- ğŸ”‘ Grain: purchase_id + effective_date
    purchase_id BIGINT NOT NULL,
    effective_date DATE NOT NULL,
    end_date DATE,  -- NULL = registro corrente
    is_current BOOLEAN NOT NULL DEFAULT FALSE,
    
    -- ğŸ“¦ Campos de purchase
    buyer_id BIGINT,
    purchase_relation_id BIGINT,
    order_date DATE,
    release_date DATE,
    producer_id BIGINT,
    purchase_value DECIMAL(10,2),
    
    -- ğŸ“¦ Campos de product_item
    product_item_id BIGINT,
    product_id BIGINT,
    item_value DECIMAL(10,2),
    
    -- ğŸ“¦ Campos de purchase_extra_info
    purchase_extra_info_id BIGINT,
    subsidiary VARCHAR(50),
    
    -- ğŸ·ï¸ Metadados
    source_update VARCHAR(100),  -- Qual tabela originou esta versÃ£o
    record_hash VARCHAR(32),     -- MD5 para detecÃ§Ã£o de mudanÃ§as
    created_at TIMESTAMP NOT NULL,
    updated_at TIMESTAMP NOT NULL,
    
    -- ğŸ” Constraints
    PRIMARY KEY (purchase_id, effective_date)
)
PARTITIONED BY (transaction_date DATE)
STORED AS PARQUET;

-- Ãndices para otimizar time travel queries
CREATE INDEX idx_current_records ON fact_purchase_history(is_current) WHERE is_current = TRUE;
CREATE INDEX idx_time_range ON fact_purchase_history(effective_date, end_date);
```

### CaracterÃ­sticas da Modelagem:

| Aspecto | ImplementaÃ§Ã£o | Justificativa |
|---------|---------------|---------------|
| **Grain** | `purchase_id` + `effective_date` | Permite mÃºltiplas versÃµes da mesma compra |
| **SCD Type** | Type 2 (versioning) | MantÃ©m histÃ³rico completo, nÃ£o sobrescreve |
| **Particionamento** | `transaction_date` | Facilita D-1 incremental e reprocessamento |
| **Flag Corrente** | `is_current = TRUE` | Queries rÃ¡pidas do estado atual |
| **Time Travel** | `effective_date` + `end_date` | Posicionar em qualquer ponto no tempo |
| **DetecÃ§Ã£o de MudanÃ§a** | MD5 hash | Evita inserir linhas duplicadas |

---

## ğŸ”„ LÃ³gica de Forward Fill

### CenÃ¡rio Real:

```python
# D-1: 2023-01-20
# Chegam eventos:
# - purchase(55): buyer_id=100, value=1000
# - product_item(55): product_id=200
# - purchase_extra_info(55): âŒ NÃƒO CHEGA

# O que fazer com subsidiary?
# OpÃ§Ã£o 1: Deixar NULL â†’ âŒ Perde informaÃ§Ã£o quando chegar
# OpÃ§Ã£o 2: Forward fill â†’ âœ… Repetir Ãºltimo valor conhecido

# Resultado em 2023-01-20:
fact_purchase_history:
  purchase_id: 55
  effective_date: 2023-01-20
  buyer_id: 100
  purchase_value: 1000
  product_id: 200
  subsidiary: NULL  # Ainda nÃ£o chegou


# D: 2023-01-23 (3 dias depois)
# Chega evento:
# - purchase_extra_info(55): subsidiary=NATIONAL

# Forward fill: Repetir campos que NÃƒO mudaram
fact_purchase_history:
  purchase_id: 55
  effective_date: 2023-01-23  # Nova versÃ£o!
  buyer_id: 100               # Repetido de 2023-01-20
  purchase_value: 1000        # Repetido de 2023-01-20
  product_id: 200             # Repetido de 2023-01-20
  subsidiary: NATIONAL        # NOVO!
```

### ImplementaÃ§Ã£o em PySpark:

```python
from pyspark.sql import Window
from pyspark.sql.functions import last, col

# Buscar Ãºltimo valor conhecido para forward fill
window_spec = Window.partitionBy("purchase_id").orderBy("effective_date")

df_with_forward_fill = df.withColumn(
    "buyer_id_filled",
    last("buyer_id", ignorenulls=True).over(window_spec)
).withColumn(
    "subsidiary_filled",
    last("subsidiary", ignorenulls=True).over(window_spec)
)
# ... para todos os campos
```

---

## âš™ï¸ Garantia de IdempotÃªncia

### Desafio:

**Como garantir que processar Janeiro/2023 10 vezes gera sempre o mesmo resultado?**

### EstratÃ©gia Implementada:

```python
def process_partition(spark, process_date):
    """
    Processa uma partiÃ§Ã£o de forma idempotente.
    
    EstratÃ©gia:
    1. DELETE da partiÃ§Ã£o existente
    2. Reconstruir do zero baseado apenas em eventos de process_date
    3. INSERT da nova partiÃ§Ã£o
    
    Resultado: Sempre determinÃ­stico!
    """
    
    # 1. Remover partiÃ§Ã£o antiga (se existir)
    spark.sql(f"""
        DELETE FROM fact_purchase_history
        WHERE transaction_date = '{process_date}'
    """)
    
    # 2. Processar eventos de process_date
    df_new_partition = build_partition_from_events(
        spark, 
        process_date,
        include_forward_fill=True
    )
    
    # 3. Inserir nova partiÃ§Ã£o
    df_new_partition.write \
        .mode("append") \
        .partitionBy("transaction_date") \
        .saveAsTable("fact_purchase_history")
    
    # 4. Atualizar flags is_current e end_date de registros antigos
    update_scd_flags(spark, process_date)
```

### Testes de IdempotÃªncia:

```python
def test_idempotency():
    """Testa que reprocessar gera o mesmo resultado."""
    
    # Processar primeira vez
    result_1 = process_partition(spark, "2023-01-20")
    hash_1 = result_1.collect().hashCode()
    
    # Processar segunda vez (reprocessamento)
    result_2 = process_partition(spark, "2023-01-20")
    hash_2 = result_2.collect().hashCode()
    
    # Processar terceira vez
    result_3 = process_partition(spark, "2023-01-20")
    hash_3 = result_3.collect().hashCode()
    
    assert hash_1 == hash_2 == hash_3, "IdempotÃªncia violada!"
```

---

## ğŸ• Time Travel (NavegaÃ§Ã£o Temporal)

### Requisito:

> "Eu preciso conseguir navegar entre perÃ­odos diferentes, tanto me posicionando no passado, como trazendo para outros momentos."

### ImplementaÃ§Ã£o:

```sql
-- GMV de Janeiro/2023 no fechamento (31/01/2023)
-- Ou seja: "Como estava no Ãºltimo dia do mÃªs?"
SELECT 
    DATE_TRUNC('month', order_date) AS month,
    SUM(purchase_value) AS gmv_total
FROM fact_purchase_history
WHERE 
    order_date >= '2023-01-01' 
    AND order_date < '2023-02-01'
    AND release_date IS NOT NULL
    -- Time Travel: Me posiciono em 31/01/2023
    AND effective_date <= '2023-01-31'
    AND (end_date > '2023-01-31' OR is_current = TRUE)
GROUP BY DATE_TRUNC('month', order_date);

-- Resultado: 100.000,00 (fechamento de janeiro)


-- GMV de Janeiro/2023 visto de Fevereiro (28/02/2023)
-- Ou seja: "Como estÃ¡ agora, considerando alteraÃ§Ãµes posteriores?"
SELECT 
    DATE_TRUNC('month', order_date) AS month,
    SUM(purchase_value) AS gmv_total
FROM fact_purchase_history
WHERE 
    order_date >= '2023-01-01' 
    AND order_date < '2023-02-01'
    AND release_date IS NOT NULL
    -- Time Travel: Me posiciono em 28/02/2023
    AND effective_date <= '2023-02-28'
    AND (end_date > '2023-02-28' OR is_current = TRUE)
GROUP BY DATE_TRUNC('month', order_date);

-- Resultado: 98.500,00 (uma compra foi estornada em fevereiro)
```

### Diagrama de Time Travel:

```
Linha do Tempo da Compra 55:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                â”‚
â”‚  2023-01-20          2023-01-23          2023-02-05           â”‚
â”‚      â”‚                   â”‚                   â”‚                 â”‚
â”‚      â–¼                   â–¼                   â–¼                 â”‚
â”‚   v1: 1000          v2: 1000          v3: 800                 â”‚
â”‚   (criada)          (+subsidiary)     (valor alterado)        â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query: GMV em 31/01/2023
  â†’ Pega v2 (effective_date <= 31/01 E end_date > 31/01)
  â†’ purchase_value = 1000

Query: GMV em 28/02/2023
  â†’ Pega v3 (effective_date <= 28/02 E end_date > 28/02 OR is_current)
  â†’ purchase_value = 800

DiferenÃ§a: 1000 - 800 = 200 (alteraÃ§Ã£o retroativa em fevereiro)
```

---

## ğŸ“ Estrutura de Arquivos

```
exercise_2_pyspark_etl/
â”œâ”€â”€ README.md (este arquivo)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl_main.py                 # Pipeline principal
â”‚   â”œâ”€â”€ transformations.py          # LÃ³gica de transformaÃ§Ã£o
â”‚   â”œâ”€â”€ data_quality.py             # ValidaÃ§Ãµes
â”‚   â””â”€â”€ utils.py                    # FunÃ§Ãµes auxiliares
â”œâ”€â”€ queries/
â”‚   â”œâ”€â”€ gmv_daily_by_subsidiary.sql
â”‚   â”œâ”€â”€ current_state.sql
â”‚   â””â”€â”€ time_travel_validation.sql
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â”œâ”€â”€ test_idempotency.py
â”‚   â””â”€â”€ test_time_travel.py
â””â”€â”€ data/
    â”œâ”€â”€ input/
    â”‚   â”œâ”€â”€ purchase_events.csv
    â”‚   â”œâ”€â”€ product_item_events.csv
    â”‚   â””â”€â”€ purchase_extra_info_events.csv
    â””â”€â”€ expected_output/
        â””â”€â”€ fact_purchase_history_sample.csv
```

---

## ğŸš€ ExecuÃ§Ã£o

```bash
# Processar eventos de uma data especÃ­fica (D-1)
python src/etl_main.py --mode process --date 2023-01-22

# Reprocessar uma partiÃ§Ã£o (idempotente)
python src/etl_main.py --mode reprocess --date 2023-01-20

# Consultar GMV com time travel
python src/etl_main.py --mode query --query-type gmv --as-of-date 2023-01-31

# Executar testes
pytest tests/ -v --cov=src
```

---

## ğŸ§ª EstratÃ©gia de Testes

### 1. Teste de Assincronismo

```python
def test_async_arrival():
    """Testa que eventos chegando em ordem diferente geram resultado correto."""
    
    # CenÃ¡rio: purchase chega D+0, product_item D+0, extra_info D+3
    events_d0 = [
        {"purchase_id": 1, "buyer_id": 100, "transaction_date": "2023-01-20"},
        {"purchase_id": 1, "product_id": 200, "transaction_date": "2023-01-20"},
    ]
    
    result_d0 = process_partition(spark, "2023-01-20", events_d0)
    assert result_d0.filter("subsidiary IS NOT NULL").count() == 0
    
    events_d3 = [
        {"purchase_id": 1, "subsidiary": "NATIONAL", "transaction_date": "2023-01-23"}
    ]
    
    result_d3 = process_partition(spark, "2023-01-23", events_d3)
    
    # Forward fill deve ter repetido buyer_id e product_id
    row = result_d3.filter("purchase_id = 1 AND effective_date = '2023-01-23'").first()
    assert row.buyer_id == 100
    assert row.product_id == 200
    assert row.subsidiary == "NATIONAL"
```

### 2. Teste de IdempotÃªncia

```python
def test_reprocessing_generates_same_result():
    """Testa que reprocessar 10x gera sempre o mesmo resultado."""
    
    checksums = []
    for i in range(10):
        result = process_partition(spark, "2023-01-20")
        checksum = result.selectExpr("md5(concat_ws('|', *))").collect()[0][0]
        checksums.append(checksum)
    
    assert len(set(checksums)) == 1, "Reprocessamento gerou resultados diferentes!"
```

### 3. Teste de Time Travel

```python
def test_time_travel_accuracy():
    """Testa que time travel retorna valores corretos do passado."""
    
    # Criar histÃ³rico com 3 versÃµes
    create_sample_data_with_changes()
    
    # Consultar GMV em 3 momentos diferentes
    gmv_jan_31 = query_gmv(as_of_date="2023-01-31")
    gmv_feb_28 = query_gmv(as_of_date="2023-02-28")
    gmv_mar_31 = query_gmv(as_of_date="2023-03-31")
    
    # Validar que valores sÃ£o diferentes (refletindo alteraÃ§Ãµes retroativas)
    assert gmv_jan_31 != gmv_feb_28  # Houve alteraÃ§Ã£o em fevereiro
    assert gmv_feb_28 == gmv_mar_31  # NÃ£o houve alteraÃ§Ã£o em marÃ§o
```

---

## ğŸ’¡ DecisÃµes de NÃ­vel SÃªnior

### 1. **Por que SCD Type 2 e nÃ£o Type 1?**

| Aspecto | Type 1 (Overwrite) | Type 2 (Versioning) | DecisÃ£o |
|---------|-------------------|---------------------|---------|
| Rastreabilidade | âŒ Perde histÃ³rico | âœ… MantÃ©m tudo | Type 2 |
| Storage | âœ… Menor | âŒ Maior | AceitÃ¡vel |
| Complexidade Query | âœ… Simples | âŒ Complexa | AceitÃ¡vel |
| Auditoria | âŒ ImpossÃ­vel | âœ… Completa | **Requisito crÃ­tico** |

**Justificativa**: Requisitos de auditoria e time travel tornam Type 2 obrigatÃ³rio.

### 2. **Por que DELETE + INSERT e nÃ£o MERGE?**

| Abordagem | PrÃ³s | Contras |
|-----------|------|---------|
| MERGE | Atualiza apenas registros alterados | Complexidade alta, dificulta debugging |
| DELETE + INSERT | Sempre gera resultado determinÃ­stico | Reescreve partiÃ§Ã£o inteira |

**DecisÃ£o**: DELETE + INSERT para garantir idempotÃªncia.

**Justificativa**: 
- PartiÃ§Ãµes diÃ¡rias sÃ£o pequenas (~1 dia de dados)
- IdempotÃªncia Ã© requisito crÃ­tico
- Debugging Ã© mais fÃ¡cil

### 3. **Por que PySpark e nÃ£o SQL?**

**Requisito do teste**: *"Se vocÃª tiver conhecimento com programaÃ§Ã£o, Python, Spark ou Scala, vocÃª possa fazer utilizando uma linguagem de programaÃ§Ã£o."*

**DecisÃ£o**: PySpark

**Justificativa**:
- âœ… Escala para grandes volumes
- âœ… Permite lÃ³gica complexa (forward fill, hash MD5)
- âœ… Suporta testes unitÃ¡rios
- âœ… IntegraÃ§Ã£o com ecossistema de dados moderno

---

## ğŸ“š ReferÃªncias e Leituras Recomendadas

- [Slowly Changing Dimensions (Kimball)](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/)
- [Event Sourcing Pattern](https://martinfowler.com/eaaDev/EventSourcing.html)
- [Idempotent Consumer](https://microservices.io/patterns/communication-style/idempotent-consumer.html)
- [PySpark Window Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html)

---

**PrÃ³ximo:** ImplementaÃ§Ã£o do cÃ³digo Python completo em [`src/etl_main.py`](./src/etl_main.py)
